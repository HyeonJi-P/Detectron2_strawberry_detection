{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "# Install detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FsePPpwZSmqt",
    "outputId": "d9aff715-74fb-490f-edc0-a59846a5195a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: python: 명령어를 찾을 수 없음\n",
      "/bin/bash: python: 명령어를 찾을 수 없음\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install pyyaml==5.1\n",
    "# Detectron2 has not released pre-built binaries for the latest pytorch (https://github.com/facebookresearch/detectron2/issues/4053)\n",
    "# so we install from source instead. This takes a few minutes.\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "\n",
    "# Install pre-built detectron2 that matches pytorch version, if released:\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "#!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/{CUDA_VERSION}/{TORCH_VERSION}/index.html\n",
    "\n",
    "# exit(0)  # After installation, you may need to \"restart runtime\" in Colab. This line can also restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d288Z2mF5dC",
    "outputId": "5e7cc5d1-5265-4e25-ae76-83aa1b747cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_Mar__8_18:18:20_PST_2022\n",
      "Cuda compilation tools, release 11.6, V11.6.124\n",
      "Build cuda_11.6.r11.6/compiler.31057947_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agtech-research/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch:  1.12 ; cuda:  cu113\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"./\")\n",
    "# Import Mask RCNNon of the library\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZyAvNCJMmvFF"
   },
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fbZ_2unzGqt-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from google.colab import drive\n",
    "drive.mount('/content/drive')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hfg9dFhqHxey"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import shutil\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def get_flower_dicts(img_dir, mode):\n",
    "    dataset_dicts = []\n",
    "    fname=[]\n",
    "    fn = []\n",
    "    i_dir = \"/home/agtech-research/바탕화면/strawberry/help/dataset/02_output/\"\n",
    "    if mode == 'train':\n",
    "        fname.append(os.path.join(i_dir, \"KETI_Strawberry_2.xml\"))\n",
    "        fname.append(os.path.join(i_dir, \"KETI_Strawberry_3.xml\"))\n",
    "        fname.append(os.path.join(i_dir, \"KETI_Strawberry_4.xml\"))\n",
    "        fname.append(os.path.join(i_dir, \"KETI_Strawberry_5.xml\"))\n",
    "        fname.append(os.path.join(i_dir, \"KETI_Strawberry_6.xml\"))\n",
    "        fname.append(os.path.join(i_dir, \"KETI_Strawberry_9.xml\"))\n",
    "        fname.append(os.path.join(i_dir, \"KETI_Strawberry_10.xml\"))\n",
    "        fname.append(os.path.join(i_dir, \"KETI_Strawberry_11.xml\"))\n",
    "        fname.append(os.path.join(i_dir, \"KETI_Strawberry_12.xml\"))\n",
    "        fn = [2,3,4,5,6,9,10,11,12]\n",
    "\n",
    "    elif mode == 'val':\n",
    "        fname.append(os.path.join(i_dir, \"KETI_Strawberry_1.xml\"))\n",
    "        fn=[1]\n",
    "\n",
    "    else :\n",
    "        fname.append(os.path.join(i_dir, \"KETI_Strawberry_7.xml\"))\n",
    "        fn=[7]\n",
    "        \n",
    "    for i in range(len(fname)):\n",
    "        anno_doc = ET.parse(fname[i])\n",
    "        annoD_root = anno_doc.getroot()\n",
    "        for items in annoD_root.iter(\"image\"):\n",
    "            filename = items.attrib[\"name\"]\n",
    "            boxlist = items.findall(\"box\")\n",
    "            \n",
    "            \n",
    "            record={}\n",
    "            record[\"file_name\"] = img_dir+str(fn[i])+\"/\"+filename\n",
    "            #print(record[\"file_name\"])\n",
    "            record[\"image_id\"] = items.attrib[\"id\"]\n",
    "            record[\"height\"] = int(items.attrib[\"height\"])\n",
    "            record[\"width\"] = int(items.attrib[\"width\"])\n",
    "\n",
    "            #print(boxlist)\n",
    "\n",
    "            objs = []\n",
    "            for bidx in range(len(boxlist)): \n",
    "                #xml에서 박스 찾기 및 저장된 속성값 가져오기\n",
    "                #occ = boxlist[bidx].attrib[\"occluded\"] #사용은 안함\n",
    "                xmax= boxlist[bidx].attrib[\"xbr\"]\n",
    "                xmin= boxlist[bidx].attrib[\"xtl\"]\n",
    "                ymax= boxlist[bidx].attrib[\"ybr\"]\n",
    "                ymin= boxlist[bidx].attrib[\"ytl\"]    \n",
    "                #z= boxlist[bidx].attrib[\"z_order\"] #사용은 안함 \n",
    "\n",
    "                px = [float(xmax), float(xmin)]\n",
    "                py = [float(ymax), float(ymin)]   \n",
    "\n",
    "                obj = {\n",
    "                        \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                        \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                        #\"segmentation\": [],\n",
    "                        \"category_id\": 0,\n",
    "                }\n",
    "                objs.append(obj)\n",
    "\n",
    "            record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "\n",
    "DatasetCatalog.clear()\n",
    "for d in [\"train\", \"val\",\"test\"]:\n",
    "    DatasetCatalog.register(\"flower_\" + d, lambda d=d: get_flower_dicts(\"/home/agtech-research/바탕화면/strawberry/help/dataset/01_rawData/KETI_Strawberry_\",d))\n",
    "    MetadataCatalog.get(\"flower_\" + d).set(thing_classes=[\"flower\"])\n",
    "\n",
    "flower_metadata = MetadataCatalog.get(\"flower_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XTrwi38qbFhO",
    "outputId": "4b6bf08b-672d-4d1f-900a-5567c57222df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset_dicts = get_flower_dicts(\"./KETI_Strawberry_1/\",\\'1\\') \\n#print(dataset_dicts)\\nfor d in random.sample(dataset_dicts, 1):\\n    img = cv2.imread(d[\"file_name\"])\\n    #print(d[\"file_name\"])\\n    v = Visualizer(img[:, :, ::-1], metadata=flower_metadata, scale=0.5)\\n    #print(\" \",d)\\n    out = v.draw_dataset_dict(d)\\n    cv2.imshow(\\'\\',out.get_image()[:, :, ::-1])'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"dataset_dicts = get_flower_dicts(\"./KETI_Strawberry_1/\",'1') \n",
    "#print(dataset_dicts)\n",
    "for d in random.sample(dataset_dicts, 1):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    #print(d[\"file_name\"])\n",
    "    v = Visualizer(img[:, :, ::-1], metadata=flower_metadata, scale=0.5)\n",
    "    #print(\" \",d)\n",
    "    out = v.draw_dataset_dict(d)\n",
    "    cv2.imshow('',out.get_image()[:, :, ::-1])\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "import os\n",
    "from detectron2.engine import DefaultTrainer\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.data import DatasetMapper, build_detection_train_loader, build_detection_test_loader   # the default mapper\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "import detectron2.utils.comm as comm\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "class LossEvalHook(HookBase):\n",
    "  def __init__(self, eval_period, model, data_loader):\n",
    "    self._model = model\n",
    "    self._period = eval_period\n",
    "    self._data_loader = data_loader\n",
    "\n",
    "  def _do_loss_eval(self):\n",
    "    # copying inference_on_dataset from evaluator.py\n",
    "    total = len(self._data_loader)\n",
    "    num_warmup = min(5, total -1)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    total_compute_time = 0\n",
    "    losses =[]\n",
    "\n",
    "    for idx, inputs in enumerate(self._data_loader):\n",
    "      if idx == num_warmup:\n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "      start_compute_time = time.perf_counter()\n",
    "      if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "      total_compute_time += time.perf_counter()- start_compute_time\n",
    "      iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "      seconds_per_image = total_compute_time / iters_after_start\n",
    "      if idx >= num_warmup * 2 or seconds_per_image > 5:\n",
    "        total_seconds_per_image = (time.perf_counter() - start_time) /iters_after_start\n",
    "        eta = datetime.timedelta(seconds=int(total_seconds_per_image * (total - idx - 1)))\n",
    "        log_every_n_seconds(\n",
    "            logging.INFO,\n",
    "            \"Loss on Validation done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
    "                idx + 1, total, seconds_per_image, str(eta)\n",
    "            ),\n",
    "            n=5,\n",
    "        )\n",
    "        loss_batch = self._get_loss(inputs)\n",
    "        losses.append(loss_batch)\n",
    "      mean_loss = np.mean(losses)\n",
    "      self.trainer.storage.put_scalar('validation_loss',mean_loss)\n",
    "      comm.synchronize()\n",
    "\n",
    "      return losses\n",
    "\n",
    "    def _get_loss(self, data):\n",
    "      # How loss is calculated on trainloop \n",
    "      metrics_dict = self._model(data)\n",
    "      metrics_dict = {\n",
    "          k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "          for k, v in metrics_dict.items()\n",
    "      }\n",
    "      total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "      return total_losses_reduced\n",
    "\n",
    "    def after_step(self):\n",
    "      next_iter = self.trainer.iter +1\n",
    "      is_final = next_iter == self.trainer.max_iter\n",
    "      if is_final or (self._period > 0 and next_iter % self._period ==0):\n",
    "        self._do_loss_eval()\n",
    "\n",
    "      self.trainer.storage.put_scalars(timetest = 12)\n",
    "\n",
    "\n",
    "\n",
    "# use this dataloader instead of the default\n",
    "\n",
    "class CustomTrainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    We use the \"DefaultTrainer\" which contains a number pre-defined logic for\n",
    "    standard training workflow. They may not work for you, especially if you\n",
    "    are working on a new research project. In that case you can use the cleaner\n",
    "    \"SimpleTrainer\", or write your own training loop.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "      if output_folder is None :\n",
    "        output_folder = \"/home/agtech-research/바탕화면/strawberry/help/detectron2/output/model_outputs_\"+str(cfg.SOLVER.BASE_LR)+\"_\"+str(cfg.SOLVER.MAX_ITER)\n",
    "      return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "\n",
    "    def build_hooks(self):\n",
    "      hooks = super().build_hooks()\n",
    "      hooks.insert(-1, LossEvalHook(\n",
    "          cfg.TEST.EVAL_PERIOD,\n",
    "          self.model,\n",
    "          build_detection_test_loader(\n",
    "              self.cfg,\n",
    "              self.cfg.DATASETS.TEST[0],\n",
    "              mapper = DatasetMapper(self.cfg,True, augmentations=[ T.Resize((640, 640)) ])\n",
    "          )\n",
    "      ))\n",
    "      return hooks\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        #dataloader = build_detection_train_loader(cfg, mapper=DatasetMapper(cfg, is_train=True, augmentations=[ T.Resize((640, 640)) ]))\n",
    "        print(\"커스텀 사용됨!\")\n",
    "        mapper=DatasetMapper(cfg, is_train=True, augmentations=[ T.Resize((640, 640)) ])\n",
    "        return build_detection_train_loader(cfg, mapper= mapper)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CnXxAuP4rULc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08/01 14:58:26 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "커스텀 사용됨!\n",
      "\u001b[32m[08/01 14:58:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [Resize(shape=(640, 640))]\n",
      "\u001b[32m[08/01 14:58:27 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 1666 images left.\n",
      "\u001b[32m[08/01 14:58:27 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   flower   | 3661         |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[08/01 14:58:27 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[08/01 14:58:27 d2.data.common]: \u001b[0mSerializing 1666 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[08/01 14:58:27 d2.data.common]: \u001b[0mSerialized dataset takes 0.89 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/01 14:58:27 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[32m[08/01 14:58:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [Resize(shape=(640, 640))]\n",
      "\u001b[32m[08/01 14:58:28 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   flower   | 366          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[08/01 14:58:28 d2.data.common]: \u001b[0mSerializing 193 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[08/01 14:58:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.10 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "root = \"/home/agtech-research/바탕화면/strawberry/help/detectron2/configs/\"\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(root+\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.DATASETS.TRAIN = (\"flower_train\",)\n",
    "cfg.DATASETS.TEST =  (\"flower_val\",)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x\")   # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2  # This is the real \"batch size\" commonly known to deep learning people,\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 500\n",
    "cfg.TEST.EVAL_PERIOD = 100 #몇 iter마다 validation 할 것인지 \n",
    "\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (flower).\n",
    "\n",
    "cfg.OUTPUT_DIR=\"/home/agtech-research/바탕화면/strawberry/help/detectron2/output/model_outputs_\"+str(cfg.SOLVER.BASE_LR)+\"_\"+str(cfg.SOLVER.MAX_ITER)+\"_\"+str(cfg.TEST.EVAL_PERIOD)\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = CustomTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ya5nEuMELeq8",
    "outputId": "244aca76-1034-4c9d-9f9f-f672c5161123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08/01 14:59:47 d2.checkpoint.c2_model_loading]: \u001b[0mFollowing weights matched with model:\n",
      "| Names in Model                                  | Names in Checkpoint                                                                                  | Shapes                                          |\n",
      "|:------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:------------------------------------------------|\n",
      "| backbone.bottom_up.res2.0.conv1.*               | backbone.bottom_up.res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,1,1)             |\n",
      "| backbone.bottom_up.res2.0.conv2.*               | backbone.bottom_up.res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n",
      "| backbone.bottom_up.res2.0.conv3.*               | backbone.bottom_up.res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
      "| backbone.bottom_up.res2.0.shortcut.*            | backbone.bottom_up.res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
      "| backbone.bottom_up.res2.1.conv1.*               | backbone.bottom_up.res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |\n",
      "| backbone.bottom_up.res2.1.conv2.*               | backbone.bottom_up.res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n",
      "| backbone.bottom_up.res2.1.conv3.*               | backbone.bottom_up.res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
      "| backbone.bottom_up.res2.2.conv1.*               | backbone.bottom_up.res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |\n",
      "| backbone.bottom_up.res2.2.conv2.*               | backbone.bottom_up.res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n",
      "| backbone.bottom_up.res2.2.conv3.*               | backbone.bottom_up.res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
      "| backbone.bottom_up.res3.0.conv1.*               | backbone.bottom_up.res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,256,1,1)       |\n",
      "| backbone.bottom_up.res3.0.conv2.*               | backbone.bottom_up.res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
      "| backbone.bottom_up.res3.0.conv3.*               | backbone.bottom_up.res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
      "| backbone.bottom_up.res3.0.shortcut.*            | backbone.bottom_up.res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (512,) (512,) (512,) (512,) (512,256,1,1)       |\n",
      "| backbone.bottom_up.res3.1.conv1.*               | backbone.bottom_up.res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n",
      "| backbone.bottom_up.res3.1.conv2.*               | backbone.bottom_up.res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
      "| backbone.bottom_up.res3.1.conv3.*               | backbone.bottom_up.res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
      "| backbone.bottom_up.res3.2.conv1.*               | backbone.bottom_up.res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n",
      "| backbone.bottom_up.res3.2.conv2.*               | backbone.bottom_up.res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
      "| backbone.bottom_up.res3.2.conv3.*               | backbone.bottom_up.res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
      "| backbone.bottom_up.res3.3.conv1.*               | backbone.bottom_up.res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n",
      "| backbone.bottom_up.res3.3.conv2.*               | backbone.bottom_up.res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
      "| backbone.bottom_up.res3.3.conv3.*               | backbone.bottom_up.res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
      "| backbone.bottom_up.res4.0.conv1.*               | backbone.bottom_up.res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,512,1,1)       |\n",
      "| backbone.bottom_up.res4.0.conv2.*               | backbone.bottom_up.res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| backbone.bottom_up.res4.0.conv3.*               | backbone.bottom_up.res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| backbone.bottom_up.res4.0.shortcut.*            | backbone.bottom_up.res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |\n",
      "| backbone.bottom_up.res4.1.conv1.*               | backbone.bottom_up.res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| backbone.bottom_up.res4.1.conv2.*               | backbone.bottom_up.res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| backbone.bottom_up.res4.1.conv3.*               | backbone.bottom_up.res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| backbone.bottom_up.res4.2.conv1.*               | backbone.bottom_up.res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| backbone.bottom_up.res4.2.conv2.*               | backbone.bottom_up.res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| backbone.bottom_up.res4.2.conv3.*               | backbone.bottom_up.res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| backbone.bottom_up.res4.3.conv1.*               | backbone.bottom_up.res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| backbone.bottom_up.res4.3.conv2.*               | backbone.bottom_up.res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| backbone.bottom_up.res4.3.conv3.*               | backbone.bottom_up.res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| backbone.bottom_up.res4.4.conv1.*               | backbone.bottom_up.res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| backbone.bottom_up.res4.4.conv2.*               | backbone.bottom_up.res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| backbone.bottom_up.res4.4.conv3.*               | backbone.bottom_up.res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| backbone.bottom_up.res4.5.conv1.*               | backbone.bottom_up.res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| backbone.bottom_up.res4.5.conv2.*               | backbone.bottom_up.res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| backbone.bottom_up.res4.5.conv3.*               | backbone.bottom_up.res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| backbone.bottom_up.res5.0.conv1.*               | backbone.bottom_up.res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,1024,1,1)      |\n",
      "| backbone.bottom_up.res5.0.conv2.*               | backbone.bottom_up.res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n",
      "| backbone.bottom_up.res5.0.conv3.*               | backbone.bottom_up.res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n",
      "| backbone.bottom_up.res5.0.shortcut.*            | backbone.bottom_up.res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |\n",
      "| backbone.bottom_up.res5.1.conv1.*               | backbone.bottom_up.res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |\n",
      "| backbone.bottom_up.res5.1.conv2.*               | backbone.bottom_up.res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n",
      "| backbone.bottom_up.res5.1.conv3.*               | backbone.bottom_up.res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n",
      "| backbone.bottom_up.res5.2.conv1.*               | backbone.bottom_up.res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |\n",
      "| backbone.bottom_up.res5.2.conv2.*               | backbone.bottom_up.res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n",
      "| backbone.bottom_up.res5.2.conv3.*               | backbone.bottom_up.res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n",
      "| backbone.bottom_up.stem.conv1.*                 | backbone.bottom_up.stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (64,) (64,) (64,) (64,) (64,3,7,7)              |\n",
      "| backbone.fpn_lateral2.*                         | backbone.fpn_lateral2.{bias,weight}                                                                  | (256,) (256,256,1,1)                            |\n",
      "| backbone.fpn_lateral3.*                         | backbone.fpn_lateral3.{bias,weight}                                                                  | (256,) (256,512,1,1)                            |\n",
      "| backbone.fpn_lateral4.*                         | backbone.fpn_lateral4.{bias,weight}                                                                  | (256,) (256,1024,1,1)                           |\n",
      "| backbone.fpn_lateral5.*                         | backbone.fpn_lateral5.{bias,weight}                                                                  | (256,) (256,2048,1,1)                           |\n",
      "| backbone.fpn_output2.*                          | backbone.fpn_output2.{bias,weight}                                                                   | (256,) (256,256,3,3)                            |\n",
      "| backbone.fpn_output3.*                          | backbone.fpn_output3.{bias,weight}                                                                   | (256,) (256,256,3,3)                            |\n",
      "| backbone.fpn_output4.*                          | backbone.fpn_output4.{bias,weight}                                                                   | (256,) (256,256,3,3)                            |\n",
      "| backbone.fpn_output5.*                          | backbone.fpn_output5.{bias,weight}                                                                   | (256,) (256,256,3,3)                            |\n",
      "| proposal_generator.rpn_head.anchor_deltas.*     | proposal_generator.rpn_head.anchor_deltas.{bias,weight}                                              | (12,) (12,256,1,1)                              |\n",
      "| proposal_generator.rpn_head.conv.*              | proposal_generator.rpn_head.conv.{bias,weight}                                                       | (256,) (256,256,3,3)                            |\n",
      "| proposal_generator.rpn_head.objectness_logits.* | proposal_generator.rpn_head.objectness_logits.{bias,weight}                                          | (3,) (3,256,1,1)                                |\n",
      "| roi_heads.box_head.fc1.*                        | roi_heads.box_head.fc1.{bias,weight}                                                                 | (1024,) (1024,12544)                            |\n",
      "| roi_heads.box_head.fc2.*                        | roi_heads.box_head.fc2.{bias,weight}                                                                 | (1024,) (1024,1024)                             |\n",
      "| roi_heads.box_predictor.bbox_pred.*             | roi_heads.box_predictor.bbox_pred.{bias,weight}                                                      | (4,) (4,1024)                                   |\n",
      "| roi_heads.box_predictor.cls_score.*             | roi_heads.box_predictor.cls_score.{bias,weight}                                                      | (2,) (2,1024)                                   |\n"
     ]
    }
   ],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)\n",
    "#outputs = predictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "koxSuGAD0hHh",
    "outputId": "81eba932-6856-4990-f172-4ebe2a836001"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@189.948] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('image_dir1/20191127_F_A-1_3.JPG'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/agtech-research/바탕화면/strawberry/help/detectron2/Detectron2_딸기꽃.ipynb 셀 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bagtech-research/home/agtech-research/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/strawberry/help/detectron2/Detectron2_%EB%94%B8%EA%B8%B0%EA%BD%83.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m random\u001b[39m.\u001b[39msample(dataset_dicts, \u001b[39m1\u001b[39m):    \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bagtech-research/home/agtech-research/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/strawberry/help/detectron2/Detectron2_%EB%94%B8%EA%B8%B0%EA%BD%83.ipynb#ch0000010vscode-remote?line=3'>4</a>\u001b[0m     im \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(d[\u001b[39m\"\u001b[39m\u001b[39mfile_name\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bagtech-research/home/agtech-research/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/strawberry/help/detectron2/Detectron2_%EB%94%B8%EA%B8%B0%EA%BD%83.ipynb#ch0000010vscode-remote?line=4'>5</a>\u001b[0m     outputs \u001b[39m=\u001b[39m predictor(im)  \u001b[39m# format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bagtech-research/home/agtech-research/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/strawberry/help/detectron2/Detectron2_%EB%94%B8%EA%B8%B0%EA%BD%83.ipynb#ch0000010vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(outputs)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bagtech-research/home/agtech-research/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/strawberry/help/detectron2/Detectron2_%EB%94%B8%EA%B8%B0%EA%BD%83.ipynb#ch0000010vscode-remote?line=6'>7</a>\u001b[0m     v \u001b[39m=\u001b[39m Visualizer(im[:, :, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bagtech-research/home/agtech-research/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/strawberry/help/detectron2/Detectron2_%EB%94%B8%EA%B8%B0%EA%BD%83.ipynb#ch0000010vscode-remote?line=7'>8</a>\u001b[0m                    metadata\u001b[39m=\u001b[39mflower_metadata, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bagtech-research/home/agtech-research/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/strawberry/help/detectron2/Detectron2_%EB%94%B8%EA%B8%B0%EA%BD%83.ipynb#ch0000010vscode-remote?line=8'>9</a>\u001b[0m                    scale\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bagtech-research/home/agtech-research/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/strawberry/help/detectron2/Detectron2_%EB%94%B8%EA%B8%B0%EA%BD%83.ipynb#ch0000010vscode-remote?line=9'>10</a>\u001b[0m                     \u001b[39m# remove the colors of unsegmented pixels. This option is only available for segmentation models\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bagtech-research/home/agtech-research/%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4/strawberry/help/detectron2/Detectron2_%EB%94%B8%EA%B8%B0%EA%BD%83.ipynb#ch0000010vscode-remote?line=10'>11</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/바탕화면/strawberry/help/detectron2/detectron2/engine/defaults.py:312\u001b[0m, in \u001b[0;36mDefaultPredictor.__call__\u001b[0;34m(self, original_image)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_format \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[39m# whether the model expects BGR inputs or RGB\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     original_image \u001b[39m=\u001b[39m original_image[:, :, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 312\u001b[0m height, width \u001b[39m=\u001b[39m original_image\u001b[39m.\u001b[39;49mshape[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    313\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maug\u001b[39m.\u001b[39mget_transform(original_image)\u001b[39m.\u001b[39mapply_image(original_image)\n\u001b[1;32m    314\u001b[0m image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(image\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "dataset_dicts = get_flower_dicts(\"image_dir\",'val')\n",
    "for d in random.sample(dataset_dicts, 1):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    print(outputs)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=flower_metadata, \n",
    "                   scale=0.5, \n",
    "                    # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    #print(outputs[\"instances\"])\n",
    "    #out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    #cv2.imshow(\" \",out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b-end : 0.09870100021362305\n",
      "b-end : 0.0989692211151123\n",
      "b-end : 0.10032463073730469\n",
      "b-end : 0.09951257705688477\n",
      "b-end : 0.09781813621520996\n",
      "b-end : 0.09846115112304688\n",
      "b-end : 0.0997769832611084\n",
      "b-end : 0.09813666343688965\n",
      "b-end : 0.09778261184692383\n",
      "b-end : 0.09917163848876953\n",
      "b-end : 0.09935665130615234\n",
      "b-end : 0.09914112091064453\n",
      "b-end : 0.09839987754821777\n",
      "b-end : 0.09906840324401855\n",
      "b-end : 0.09741425514221191\n",
      "b-end : 0.10317707061767578\n",
      "b-end : 0.09901189804077148\n",
      "b-end : 0.09861564636230469\n",
      "b-end : 0.09951186180114746\n",
      "b-end : 0.09824156761169434\n",
      "b-end : 0.09892392158508301\n",
      "b-end : 0.09811830520629883\n",
      "b-end : 0.09811568260192871\n",
      "b-end : 0.09911394119262695\n",
      "b-end : 0.09913992881774902\n",
      "b-end : 0.0987997055053711\n",
      "b-end : 0.09800887107849121\n",
      "b-end : 0.09972739219665527\n",
      "b-end : 0.0978245735168457\n",
      "b-end : 0.09841752052307129\n",
      "b-end : 0.10141873359680176\n",
      "b-end : 0.09929060935974121\n",
      "b-end : 0.10191798210144043\n",
      "b-end : 0.10025572776794434\n",
      "b-end : 0.09858393669128418\n",
      "b-end : 0.09960556030273438\n",
      "b-end : 0.09829092025756836\n",
      "b-end : 0.0997459888458252\n",
      "b-end : 0.09889435768127441\n",
      "b-end : 0.09749746322631836\n",
      "b-end : 0.09718441963195801\n",
      "b-end : 0.09815025329589844\n",
      "b-end : 0.0994408130645752\n",
      "b-end : 0.09867429733276367\n",
      "b-end : 0.10249948501586914\n",
      "b-end : 0.10016202926635742\n",
      "b-end : 0.09807944297790527\n",
      "b-end : 0.09908747673034668\n",
      "b-end : 0.09864497184753418\n",
      "b-end : 0.09881043434143066\n",
      "b-end : 0.09893465042114258\n",
      "b-end : 0.10079550743103027\n",
      "b-end : 0.10155987739562988\n",
      "b-end : 0.10098028182983398\n",
      "b-end : 0.09888315200805664\n",
      "b-end : 0.09824109077453613\n",
      "b-end : 0.09938526153564453\n",
      "b-end : 0.0989687442779541\n",
      "b-end : 0.0987546443939209\n",
      "b-end : 0.1001126766204834\n",
      "b-end : 0.10156798362731934\n",
      "b-end : 0.09907889366149902\n",
      "b-end : 0.09798002243041992\n",
      "b-end : 0.09999513626098633\n",
      "b-end : 0.09886860847473145\n",
      "b-end : 0.09830093383789062\n",
      "b-end : 0.09892654418945312\n",
      "b-end : 0.09975028038024902\n",
      "b-end : 0.09852886199951172\n",
      "b-end : 0.10227179527282715\n",
      "b-end : 0.09760546684265137\n",
      "b-end : 0.09909272193908691\n",
      "b-end : 0.0992441177368164\n",
      "b-end : 0.0975654125213623\n",
      "b-end : 0.09845256805419922\n",
      "b-end : 0.09985589981079102\n",
      "b-end : 0.09880471229553223\n",
      "b-end : 0.09953784942626953\n",
      "b-end : 0.09879875183105469\n",
      "b-end : 0.10211586952209473\n",
      "b-end : 0.09875106811523438\n",
      "b-end : 0.09999203681945801\n",
      "b-end : 0.09890317916870117\n",
      "b-end : 0.0980215072631836\n",
      "b-end : 0.09828591346740723\n",
      "b-end : 0.09883594512939453\n",
      "b-end : 0.09925031661987305\n",
      "b-end : 0.09825873374938965\n",
      "b-end : 0.10051369667053223\n",
      "b-end : 0.09961533546447754\n",
      "b-end : 0.09924936294555664\n",
      "b-end : 0.09878230094909668\n",
      "b-end : 0.09759283065795898\n",
      "b-end : 0.09907793998718262\n",
      "b-end : 0.09770011901855469\n",
      "FPS:  20.485050467960914\n"
     ]
    }
   ],
   "source": [
    "# inference time 측정\n",
    "import time\n",
    "\n",
    "dataset_dicts = get_flower_dicts(\"/home/agtech-research/바탕화면/strawberry/help/dataset/01_rawData/KETI_Strawberry_\",'test')\n",
    "flag = 0\n",
    "for d in dataset_dicts :  \n",
    "    if flag == 0 : \n",
    "        start = time.time()\n",
    "        flag = 1 \n",
    "    before_read = time.time() \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    after_read = time.time()\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    end = time.time()\n",
    "    #print(outputs)\n",
    "    print(\"b-end :\",end - before_read) #평균 0.1\n",
    "    \n",
    "    \n",
    "print(\"FPS: \", 193.0 / (end - start)) # 약 0.104\n",
    "print((end - start)/60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4LiA2knOZfl",
    "outputId": "2147c68f-2d04-47e0-eb7d-da7f4f662d54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08/01 13:15:37 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[32m[08/01 13:15:37 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'flower_test' to COCO format ...\n",
      "\u001b[32m[08/01 13:15:37 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'flower_test' to COCO format ...)\n",
      "\u001b[32m[08/01 13:15:37 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n",
      "\u001b[32m[08/01 13:15:37 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 95, #annotations: 196\n",
      "\u001b[32m[08/01 13:15:37 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at './output/flower_test_coco_format.json' ...\n",
      "\u001b[32m[08/01 13:15:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [Resize(shape=(640, 640))]\n",
      "\u001b[32m[08/01 13:15:37 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   flower   | 196          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[08/01 13:15:37 d2.data.common]: \u001b[0mSerializing 95 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[08/01 13:15:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.05 MiB\n",
      "\u001b[32m[08/01 13:15:37 d2.evaluation.evaluator]: \u001b[0mStart inference on 95 batches\n",
      "\u001b[32m[08/01 13:15:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/95. Dataloading: 0.0221 s/iter. Inference: 0.0212 s/iter. Eval: 0.0001 s/iter. Total: 0.0434 s/iter. ETA=0:00:03\n",
      "\u001b[32m[08/01 13:15:41 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:04.058566 (0.045095 s / iter per device, on 1 devices)\n",
      "\u001b[32m[08/01 13:15:41 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:01 (0.020628 s / iter per device, on 1 devices)\n",
      "\u001b[32m[08/01 13:15:41 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[08/01 13:15:41 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[08/01 13:15:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.717\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.859\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.820\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.194\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.745\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.399\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.751\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.751\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.300\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.777\n",
      "\u001b[32m[08/01 13:15:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 71.746 | 85.888 | 82.040 |  nan  | 19.434 | 74.511 |\n",
      "\u001b[32m[08/01 13:15:41 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "OrderedDict([('bbox', {'AP': 71.74641504993225, 'AP50': 85.88848320393109, 'AP75': 82.03991360781751, 'APs': nan, 'APm': 19.434229137199434, 'APl': 74.51072857230268})])\n"
     ]
    }
   ],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"flower_test\", output_dir=\"./output\")\n",
    "val_loader = build_detection_test_loader(cfg, \"flower_test\", mapper = DatasetMapper(cfg,True, augmentations=[ T.Resize((640, 640)) ]))\n",
    "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
    "# another equivalent way to evaluate the model is to use `trainer.test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Detectron2_딸기꽃.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
